python get_tokenization.py \
	--corpus "/data/xuht/websiteanalyze-data-seqing20180821/yancao.txt" \
	--model_prefix "/data/xuht/product/yancao/sentence_piece_" \
	--vocab_size 50000 \
	--model_type "char" \
	--character_coverage 0.9995 \
